{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290aa860-d2a1-492d-82ec-2d9d127c735e",
   "metadata": {},
   "source": [
    "# Document's Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d6c0c-3f2a-405d-8553-ef34b88ec6e6",
   "metadata": {},
   "source": [
    "#### · Display raw data from cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb031946-3161-493e-9ec1-04a4741f9e49",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### · Display raw data and located peaks recorded in cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc7aa9-0dc4-4215-9abf-097d9c171732",
   "metadata": {},
   "source": [
    "#### · Find peaks of recorded raw data with the most updated version of the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8be0e4-fdde-4314-81b1-347a2738249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import nirscloud_util_meta\n",
    "import nirscloud_util_hdfs\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import pyspark\n",
    "import sys\n",
    "from plotly.subplots import make_subplots\n",
    "import math \n",
    "import plotly.graph_objects as go\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import matrix\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import interpolate\n",
    "import datetime\n",
    "from scipy.signal import find_peaks,butter, lfilter, lfilter_zi, convolve,resample, correlate, iirnotch, filtfilt, stft\n",
    "from scipy import signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b991332-290e-4327-83ab-27b2d45b062a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pymongo==3.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f41072d9-9310-4bd7-a3c8-3dbdd8295ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be889b9-72ff-482f-ada7-345a782889d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_name = 'braulio.ramirez'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf36577a-f81f-42b0-a956-ec62464acdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('jhub')\n",
    "hostname = 'mongos.mongo.svc.cluster.local:27017'\n",
    "pemkeyfile = '/etc/mongo/jhub-keypem.pem'\n",
    "sslca = '/etc/mongo/root-ca.pem'\n",
    "\n",
    "nirscloud_util_meta.init(logger, 'meta', hostname=hostname, ssl=True, cert=pemkeyfile, ca=sslca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784166a8-3bb9-4462-bdec-d9255df1654c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 19:30:26 [INFO ] cfg#_init_hdfs_kinit@79: after hdfs_kinit: the_stdout: b'' the_stderr: b''\n",
      "2023-06-30 19:30:26 [INFO ] client#__init__@192: Instantiated <KerberosClient(url='https://hdfs2.babynirs.org:9870;https://hdfs1.babynirs.org:9870;https://hdfs4.babynirs.org:9870')>.\n"
     ]
    }
   ],
   "source": [
    "spark_kerberos_principal = my_name + '@BABYNIRS.ORG'\n",
    "\n",
    "params = {\n",
    "    'spark_kerberos_principal': spark_kerberos_principal,\n",
    "}\n",
    "nirscloud_util_hdfs.init('/etc/jhub/conf/production.ini', params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9682083-9c01-4a9d-bc69-34bb8224669e",
   "metadata": {},
   "source": [
    "### Pulling the recorded peaks from the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829c37dc-48bc-4c72-a51f-47f6488b4a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdfs_path = '_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5848ff80-2731-4aff-b3bd-0e26d800f5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_topics = 'nk_rpeak2_NICU'\n",
    "hdfs_prefix = '/kafka/topics/%s'% (kafka_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9443025f-cfe8-4370-879b-797b1a25bf86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_df(hdfs_path, the_id):\n",
    "    hdfs_path = hdfs_path\n",
    "    full_path = nirscloud_util_hdfs.full_path(hdfs_prefix, hdfs_path)\n",
    "    err, df = nirscloud_util_hdfs.from_hdfs_path(full_path)\n",
    "    print(err, df)\n",
    "    is_valid = df['id'] == the_id\n",
    "    df = df[is_valid]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc7b90a-c015-4b49-9375-b1d854cc8744",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 19:30:26 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929'.\n",
      "2023-06-30 19:30:27 [INFO ] client#list@1123: Listing '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929'.\n",
      "2023-06-30 19:30:27 [INFO ] client#list@1123: Listing '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00000-3dc1ad61-1e54-40f7-bf0c-9dd31613338c.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:27 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00000-3dc1ad61-1e54-40f7-bf0c-9dd31613338c.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00000-a7de3d7b-e862-44e6-92e4-3c5f614da7c3.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:27 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00000-a7de3d7b-e862-44e6-92e4-3c5f614da7c3.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00001-5c9b6e80-d106-4e98-8785-30fac20ce5d9.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:27 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00001-5c9b6e80-d106-4e98-8785-30fac20ce5d9.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:27 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00001-7d8f0b76-fb44-42de-b9b2-c872825e306e.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:27 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00001-7d8f0b76-fb44-42de-b9b2-c872825e306e.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00001-ffd4be5c-52ca-4aa1-a4e7-ae7835c7b982.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00001-ffd4be5c-52ca-4aa1-a4e7-ae7835c7b982.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00002-b1b22d74-6344-4835-a54d-e4f13c127cc5.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00002-b1b22d74-6344-4835-a54d-e4f13c127cc5.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00002-d984a888-fd97-424c-ad25-24edaf49eb1f.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00002-d984a888-fd97-424c-ad25-24edaf49eb1f.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00003-a9d4f8c2-0090-480d-9297-dbda0f436f70.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00003-a9d4f8c2-0090-480d-9297-dbda0f436f70.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00003-f3cdcf06-7be1-43a9-a32a-c6e9decbf73d.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00003-f3cdcf06-7be1-43a9-a32a-c6e9decbf73d.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00004-17d15225-8719-4953-ad45-73ed1dd42460.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00004-17d15225-8719-4953-ad45-73ed1dd42460.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:28 [INFO ] client#status@320: Fetching status for '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00004-b112fd52-76d1-4ffd-b27c-fa89fb2c521d.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:28 [INFO ] client#read@731: Reading file '/kafka/topics/nk_rpeak2_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/part-00004-b112fd52-76d1-4ffd-b27c-fa89fb2c521d.c000.snappy.parquet'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None       id             start_ns          rec_nano_ts       val      ver  \\\n",
      "0     II  1686372038314000000  1686372039283320820  1.149853  v0.0.23   \n",
      "1     II  1686372038826000000  1686372039623391041  0.070611  v0.0.23   \n",
      "2     II  1686372038826000000  1686372039963980267  0.471633  v0.0.23   \n",
      "3     II  1686372039338000000  1686372040304866902  0.387760  v0.0.23   \n",
      "4     II  1686372039850000000  1686372040645622712  0.322574  v0.0.23   \n",
      "...   ..                  ...                  ...       ...      ...   \n",
      "3476  II  1686373175978000000  1686373176977949007 -0.491993  v0.0.23   \n",
      "3477  II  1686373176490000000  1686373177586859349  0.641429  v0.0.23   \n",
      "3478  II  1686373177002000000  1686373178182592895  0.716261  v0.0.23   \n",
      "3479  II  1686373177514000000  1686373178621707140  0.550078  v0.0.23   \n",
      "3480  II  1686373178026000000  1686373179078254610  2.296289  v0.0.23   \n",
      "\n",
      "     _device_id  _bed_id   _the_date _hr _patient_id  \n",
      "0       Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "1       Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "2       Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "3       Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "4       Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "...         ...      ...         ...  ..         ...  \n",
      "3476    Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "3477    Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "3478    Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "3479    Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "3480    Procyon  HA11-01  2023-06-10  00     5984929  \n",
      "\n",
      "[3481 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "df_II_peaks = _get_df(hdfs_path, 'II')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a745a74-9e1b-4c14-a126-7c116801485d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_II_peaks.sort_values(by=['rec_nano_ts'], inplace=True)\n",
    "df_II_peaks.reset_index(drop=True, inplace=True)\n",
    "df_II_peaks['shift_rec_nano_ts'] = df_II_peaks['rec_nano_ts'].shift(-1)\n",
    "df_II_peaks['dif_rec_nano_ts'] = df_II_peaks['shift_rec_nano_ts'] - df_II_peaks['rec_nano_ts']\n",
    "df_II_peaks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5bc05c0-5f80-49c0-82f5-954c40710c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_p = np.asarray(df_II_peaks['val'].to_numpy())\n",
    "raw_time_p = np.asarray(df_II_peaks['rec_nano_ts'].to_numpy(), dtype = 'int')\n",
    "time_p = [np.datetime64(int(t),'ns') for t in raw_time_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd620ff-217c-4393-a6f2-ad9e9d181d42",
   "metadata": {},
   "source": [
    "## Pulling continuous signal data from cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "470ad255-978b-484c-b696-300861add7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_topics_cs = 'nk_waves_NICU'\n",
    "hdfs_prefix_cs = '/nirscloud/agg_by_hr3/%s'% (kafka_topics_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3983926-1996-4999-b07f-25754dcae52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_df_cs(hdfs_path, the_id):\n",
    "    hdfs_path = hdfs_path + '/id=%s' % (the_id)\n",
    "    full_path = nirscloud_util_hdfs.full_path(hdfs_prefix_cs, hdfs_path)\n",
    "    err, df = nirscloud_util_hdfs.from_hdfs_path(full_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f926607-7c77-4373-bc88-ebb12881fa2f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 19:30:29 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:29 [INFO ] client#status@320: Fetching status for '/nirscloud/agg_by_hr3/nk_waves_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/id=MDC_ECG_ELEC_POTL_II'.\n",
      "2023-06-30 19:30:29 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:29 [INFO ] client#status@320: Fetching status for '/nirscloud/agg_by_hr3/nk_waves_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/id=MDC_ECG_ELEC_POTL_II'.\n",
      "2023-06-30 19:30:29 [INFO ] client#list@1123: Listing '/nirscloud/agg_by_hr3/nk_waves_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/id=MDC_ECG_ELEC_POTL_II'.\n",
      "2023-06-30 19:30:29 [INFO ] client#status@320: Fetching status for '/nirscloud/agg_by_hr3/nk_waves_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/id=MDC_ECG_ELEC_POTL_II'.\n",
      "2023-06-30 19:30:29 [INFO ] client#list@1123: Listing '/nirscloud/agg_by_hr3/nk_waves_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/id=MDC_ECG_ELEC_POTL_II'.\n",
      "2023-06-30 19:30:29 [INFO ] client#status@320: Fetching status for '/nirscloud/agg_by_hr3/nk_waves_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/id=MDC_ECG_ELEC_POTL_II'.\n",
      "2023-06-30 19:30:29 [INFO ] client#status@320: Fetching status for '/'.\n",
      "2023-06-30 19:30:29 [INFO ] client#status@320: Fetching status for '/nirscloud/agg_by_hr3/nk_waves_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/id=MDC_ECG_ELEC_POTL_II/part-00000-1f75843e-e8df-43ac-84d1-1c5b94653a8c.c000.snappy.parquet'.\n",
      "2023-06-30 19:30:29 [INFO ] client#read@731: Reading file '/nirscloud/agg_by_hr3/nk_waves_NICU/_device_id=Procyon/_bed_id=HA11-01/_the_date=2023-06-10/_hr=00/_patient_id=5984929/id=MDC_ECG_ELEC_POTL_II/part-00000-1f75843e-e8df-43ac-84d1-1c5b94653a8c.c000.snappy.parquet'.\n"
     ]
    }
   ],
   "source": [
    "df_II_cs = _get_df_cs(hdfs_path, 'MDC_ECG_ELEC_POTL_II')\n",
    "df_II_cs.sort_values(by=['_milli_ts'], inplace=True)\n",
    "df_II_cs.reset_index(drop=True, inplace=True)\n",
    "df_II_cs['shift_milli_ts'] = df_II_cs['_milli_ts'].shift(-1)\n",
    "df_II_cs['diff_milli_ts'] = df_II_cs['shift_milli_ts'] - df_II_cs['_milli_ts']\n",
    "df_II_cs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdd81ff9-3979-477d-a917-eb7eb5835489",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_cs = np.asarray(df_II_cs['val'].to_numpy())\n",
    "raw_time_cs = np.asarray(df_II_cs['_milli_ts'].to_numpy(), dtype = 'int')\n",
    "time_cs = [np.datetime64(int(t),'ms') for t in raw_time_cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a98a6802-808f-41b5-8189-97ae411966a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pks = np.array([(pd.Timestamp(t).hour*60*60*1000) + (pd.Timestamp(t).minute*60*1000) + (pd.Timestamp(t).second*1000) + (pd.Timestamp(t).microsecond/1000) for t in time_p]);\n",
    "time_cts = np.array([(pd.Timestamp(t).hour*60*60*1000) + (pd.Timestamp(t).minute*60*1000) + (pd.Timestamp(t).second*1000) + (pd.Timestamp(t).microsecond/1000) for t in time_cs]);\n",
    "time_pks = time_pks - time_cts[0]; time_cts = time_cts - time_cts[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "646078d7-b988-4a50-be5a-55b4d520a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_f(time_array):\n",
    "\n",
    "    mins = ((time_array/1000)/60).astype(int);\n",
    "    sec = ((time_array/1000)%60).astype(int);\n",
    "    mili = ((time_array - (mins*60*1000 + sec*1000))*100).astype(int)/100 #*100.astype(int)/100 is to round to two decimals\n",
    "\n",
    "    return (np.array([str(minutes)+':'+str(seconds)+':'+str(milis) for minutes, seconds, milis in zip(mins,sec,mili)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c71380b0-934a-495f-8516-cba2ce8850a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class raw_sample_():\n",
    "    \n",
    "    def __init__(self,min_min,min_sec,max_min,max_sec,seconds_per_label):\n",
    "        \n",
    "        self.min_min = min_min\n",
    "        self.min_sec = min_sec\n",
    "        self.max_min = max_min\n",
    "        self.max_sec = max_sec\n",
    "        self.seconds_per_label = seconds_per_label\n",
    "\n",
    "        self.min_time = int((self.min_min*60*1000)/4) + int(self.min_sec*1000/4)\n",
    "        self.max_time = int((self.max_min*60*1000)/4) + int(self.max_sec*1000/4) + 1\n",
    "        self.pks_time_min = (self.min_min*60*1000) + (self.min_sec*1000)\n",
    "        self.pks_time_max = (self.max_min*60*1000) + (self.max_sec*1000)\n",
    "        self.pks_time = time_pks[(self.pks_time_min > time_pks).argmin():(time_pks < self.pks_time_max).argmin()]\n",
    "        self.pks_values = values_p[(self.pks_time_min > time_pks).argmin():(time_pks < self.pks_time_max).argmin()]\n",
    "        \n",
    "        self.time =time_cts[self.min_time:self.max_time]\n",
    "        self.values = df_II_cs['val'].to_numpy()[self.min_time:self.max_time]\n",
    "\n",
    "        self.fig = go.Figure()\n",
    "        self.fig.add_trace(go.Scatter(line= dict(color='blue'),x=self.time, y=self.values,showlegend=False))\n",
    "        self.fig.add_trace(go.Scatter(mode='markers', marker= dict(color='red'),x=self.pks_time, y=self.pks_values,showlegend=False))\n",
    "\n",
    "        self.ticks_values = []\n",
    "        x = self.time[0]\n",
    "        while x < self.time[-1]:\n",
    "            self.ticks_values = np.append(self.ticks_values,x)\n",
    "            x = 1000*self.seconds_per_label + x\n",
    "\n",
    "        self.time_format_axis = time_f(self.ticks_values)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551947f-9752-4262-a7e8-73b8fdb639d6",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c788685-be82-487a-812f-85f74f012b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_():\n",
    "    \n",
    "    def __init__(self,min_min,min_sec,max_min,max_sec,seconds_per_label,find_the_peaks,inverted):\n",
    "        \n",
    "        self.min_min = min_min\n",
    "        self.min_sec = min_sec\n",
    "        self.max_min = max_min\n",
    "        self.max_sec = max_sec\n",
    "        self.seconds_per_label = seconds_per_label\n",
    "\n",
    "        self.min_time = int((self.min_min*60*1000)/4 + int(self.min_sec*1000)/4) \n",
    "        self.max_time = int((self.max_min*60*1000)/4 + int(self.max_sec*1000)/4) + 1\n",
    "\n",
    "        \n",
    "        time_zero = raw_time_cs[0]\n",
    "        self.time = raw_time_cs[self.min_time:self.max_time] - time_zero\n",
    "        if inverted:\n",
    "            self.values = - df_II_cs['val'].to_numpy()[self.min_time:self.max_time]\n",
    "        else: \n",
    "            self.values =  df_II_cs['val'].to_numpy()[self.min_time:self.max_time]\n",
    "\n",
    "        self.fig = go.Figure()\n",
    "        self.fig.add_trace(\n",
    "                      go.Scatter(line= dict(color='blue'),x=self.time, y=self.values,showlegend=False))\n",
    "\n",
    "        self.ticks_values = []\n",
    "        x = self.time[0]\n",
    "        while x < self.time[-1]:\n",
    "            self.ticks_values = np.append(self.ticks_values,x)\n",
    "            x = 1000*self.seconds_per_label + x\n",
    "\n",
    "        self.time_format_axis = time_f(self.ticks_values)\n",
    "        \n",
    "        if find_the_peaks:\n",
    "            self.peaks, _ = find_peaks(self.values, distance=100)\n",
    "            self.fig.add_trace(\n",
    "                      go.Scatter(mode='markers',x=self.time[self.peaks], y=self.values[self.peaks],showlegend=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdedbac1-5213-4e3d-bbd9-c2b0c3b79870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs_r_window = baseline removal window;     #fs = sampling frequency                                      #trans_width = transition width of bandpass \n",
    "#numtaps = size of filter                   # mwi_windoow = moving window integration window\n",
    "\n",
    "\n",
    "\n",
    "class sample_all_included():\n",
    "    \n",
    "    first_time = True\n",
    "    \n",
    "    def __init__(self,min_min,min_sec,max_min,max_sec,seconds_per_label,inverted, bs_r_window,band_start,band_stop,fs,trans_width,numtaps, \n",
    "                 mwi_window, test_segments,spline,res):\n",
    "        \n",
    "        self.min_min = min_min\n",
    "        self.min_sec = min_sec\n",
    "        self.max_min = max_min\n",
    "        self.max_sec = max_sec\n",
    "        self.seconds_per_label = seconds_per_label\n",
    "\n",
    "\n",
    "        self.min_time = int((self.min_min*60*1000)/4 + int(self.min_sec*1000)/4) \n",
    "        self.max_time = int((self.max_min*60*1000)/4 + int(self.max_sec*1000)/4) + 1\n",
    "\n",
    "                \n",
    "        time_zero = raw_time_cs[0]\n",
    "        self.time = raw_time_cs[self.min_time:self.max_time] - time_zero\n",
    "        if inverted:\n",
    "            self.values = - df_II_cs['val'].to_numpy()[self.min_time:self.max_time]\n",
    "        else: \n",
    "            self.values =  df_II_cs['val'].to_numpy()[self.min_time:self.max_time]\n",
    "        \n",
    "       \n",
    "        self.ticks_values = []\n",
    "        x = self.time[0]\n",
    "        while x < self.time[-1]:\n",
    "            self.ticks_values = np.append(self.ticks_values,x)\n",
    "            x = 1000*self.seconds_per_label + x\n",
    "\n",
    "        self.time_format_axis = time_f(self.ticks_values)       \n",
    "        \n",
    "        #Filters' Parameters\n",
    "        self.first_bs_r_window = bs_r_window\n",
    "        self.first_band = [band_start, band_stop]\n",
    "        self.first_mwi_window = mwi_window\n",
    "        self.bs_r_window = bs_r_window\n",
    "        self.trans_width = trans_width\n",
    "        self.numtaps = numtaps\n",
    "        self.fs = fs\n",
    "        self.band = [band_start,band_stop]\n",
    "        self.edges = [0, self.band[0] - trans_width, self.band[0], self.band[1], self.band[1] + self.trans_width, 0.5*fs]\n",
    "        self.test_segments = test_segments\n",
    "        self.segment_duration = 0.8\n",
    "        self.spline = spline \n",
    "        self.res = res\n",
    "        \n",
    "\n",
    "        # Preprocessed Data    \n",
    "        #self.bs_re, self.cut_pts = moving_average(self.bs_r_window, self.values)\n",
    "        #self.prepro_data =  self.values[0:len(self.time)-self.cut_pts] - self.bs_re\n",
    "        #self.prepro_x = self.time[0:len(self.time)-self.cut_pts]\n",
    "        \n",
    "        #Bandpass\n",
    "        self.taps = signal.remez(self.numtaps, self.edges, [0, 1, 0], fs=self.fs)\n",
    "        self.bp_x_in = np.append(self.prepro_data,np.zeros(len(self.taps)))\n",
    "        self.bp_y = np.zeros(len(self.prepro_data)+len(self.taps))\n",
    "        \n",
    "        for j in range(0,len(self.prepro_data)+len(self.taps)): \n",
    "            sum = 0\n",
    "            for k in range(0,len(self.taps)):\n",
    "                sum = (self.bp_x_in[j-k]*self.taps[k]) + sum\n",
    "            self.bp_y[j] = sum\n",
    "            \n",
    "        self.bp_y=np.delete(self.bp_y,[np.arange(0,int(len(self.taps)/2))])\n",
    "        self.bp_x = self.time[0:len(self.bp_y)]\n",
    "\n",
    "        \n",
    "        #derivative\n",
    "        self.der_y = np.zeros(len(self.bp_y))\n",
    "        self.der_x = self.bp_x\n",
    "        self.der_x_in = np.append(self.bp_y,np.zeros(2))\n",
    "\n",
    "        for i in range(0,len(self.bp_y)):\n",
    "            self.der_y[i] = (1/8) * (-(self.der_x_in[i-2]) - (2*self.der_x_in[i-1]) + (2*self.der_x_in[i+1]) + (self.der_x_in[i+2]))\n",
    "        \n",
    "        #squaring\n",
    "        self.sq_y = (self.der_y) ** 2\n",
    "        self.sq_x = self.der_x\n",
    "        \n",
    "        #Moving Window Integration\n",
    "        self.mwi_window = mwi_window\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def bs_r_window(self): \n",
    "        return self._bs_r_window\n",
    "\n",
    "    @bs_r_window.setter\n",
    "    def bs_r_window(self,new_window_value):\n",
    "        self._bs_r_window = new_window_value\n",
    "        self.bs_re, self.cut_pts = moving_average(self._bs_r_window,self.values)\n",
    "        self.prepro_data = self.values[0:len(self.time)-self.cut_pts] - self.bs_re\n",
    "        self.prepro_x = self.time[0:len(self.time)-self.cut_pts]\n",
    "        \n",
    "        if (new_window_value != self.first_bs_r_window): \n",
    "            self.first_bs_r_window = 300\n",
    "            self.bs_re, self.cut_pts = moving_average(self._bs_r_window,self.values)\n",
    "            self.prepro_data = self.values[0:len(self.time)-self.cut_pts] - self.bs_re\n",
    "            self.prepro_x = self.time[0:len(self.time)-self.cut_pts]\n",
    "            \n",
    "            self.edges = [0, self.band[0] - self.trans_width, self.band[0], self.band[1], self.band[1] + self.trans_width, 0.5*self.fs]\n",
    "            self.taps = signal.remez(self.numtaps, self.edges, [0, 1, 0], fs=self.fs)\n",
    "            self.bp_x_in = np.append(self.prepro_data,np.zeros(len(self.taps)))\n",
    "            self.bp_y = np.zeros(len(self.prepro_data)+len(self.taps))\n",
    "            \n",
    "            for j in range(0,len(self.prepro_data)+len(self.taps)): \n",
    "                sum = 0\n",
    "                for k in range(0,len(self.taps)):\n",
    "                    sum = (self.bp_x_in[j-k]*self.taps[k]) + sum\n",
    "                self.bp_y[j] = sum\n",
    "\n",
    "            self.bp_y=np.delete(self.bp_y,[np.arange(0,int(len(self.taps)/2))])\n",
    "            self.bp_x = self.time[0:len(self.bp_y)]\n",
    "            \n",
    "            #Derivative when bandpass is updated\n",
    "            self.der_y = np.zeros(len(self.bp_y))\n",
    "            self.der_x_in = np.append(self.bp_y,np.zeros(2))\n",
    "            for i in range(0,len(self.bp_y)):\n",
    "                self.der_y[i] = (1/8) * (-(self.der_x_in[i-2]) - (2*self.der_x_in[i-1]) + (2*self.der_x_in[i+1]) + (self.der_x_in[i+2]))\n",
    "\n",
    "            self.der_x = self.bp_x\n",
    "\n",
    "            #Squaring when bandpass is updatd\n",
    "            self.sq_y = self.der_y ** 2\n",
    "            self.sq_x = self.der_x\n",
    "\n",
    "            #Movig window integgration when bandpass is updated \n",
    "            self.mwi_y = dif_eq_window_integration(self.sq_y,self.mwi_window,False)\n",
    "            self.mwi_x = self.der_x\n",
    "\n",
    "            #Fiducial points when bandpass is updated\n",
    "            self.fiducial_points = find_fiducial_point(self.mwi_y,self.mwi_x,self.test_segments,self.segment_duration,self.mwi_window)\n",
    "            self.max_x, self.max_y, self.peaks_x, self.peaks_y, self.interpolated_peak_x, self.interpolated_peak_y,self.max_ids = interpolation_spline(\n",
    "                self.fiducial_points, self.time, self.values, self.spline, self.res)\n",
    "\n",
    "            \n",
    "        \n",
    "    @property\n",
    "    def band(self):\n",
    "        return self._band\n",
    "    \n",
    "    @band.setter\n",
    "    def band(self,new_band):\n",
    "        self.band_start = new_band[0]; self.band_stop = new_band[1]\n",
    "        self._band = [new_band[0],new_band[1]]\n",
    "        if (new_band != self.first_band): \n",
    "            self.first_band = 0.01\n",
    "            self.edges = [0, self._band[0] - self.trans_width, self._band[0], self._band[1], self._band[1] + self.trans_width, 0.5*self.fs]\n",
    "            self.taps = signal.remez(self.numtaps, self.edges, [0, 1, 0], fs=self.fs)\n",
    "            self.bp_x_in = np.append(self.prepro_data,np.zeros(len(self.taps)))\n",
    "            self.bp_y = np.zeros(len(self.prepro_data)+len(self.taps))\n",
    "\n",
    "            for j in range(0,len(self.prepro_data)+len(self.taps)): \n",
    "                sum = 0\n",
    "                for k in range(0,len(self.taps)):\n",
    "                    sum = (self.bp_x_in[j-k]*self.taps[k]) + sum\n",
    "                self.bp_y[j] = sum\n",
    "\n",
    "            self.bp_y=np.delete(self.bp_y,[np.arange(0,int(len(self.taps)/2))])\n",
    "            self.bp_x = self.time[0:len(self.bp_y)]\n",
    "\n",
    "\n",
    "            #Derivative when bandpass is updated\n",
    "            self.der_y = np.zeros(len(self.bp_y))\n",
    "            self.der_x_in = np.append(self.bp_y,np.zeros(2))\n",
    "            for i in range(0,len(self.bp_y)):\n",
    "                self.der_y[i] = (1/8) * (-(self.der_x_in[i-2]) - (2*self.der_x_in[i-1]) + (2*self.der_x_in[i+1]) + (self.der_x_in[i+2]))\n",
    "\n",
    "            self.der_x = self.bp_x\n",
    "\n",
    "            #Squaring when bandpass is updatd\n",
    "            self.sq_y = self.der_y ** 2\n",
    "            self.sq_x = self.der_x\n",
    "\n",
    "            #Movig window integgration when bandpass is updated \n",
    "            self.mwi_y = dif_eq_window_integration(self.sq_y,self.mwi_window,False)\n",
    "            self.mwi_x = self.der_x\n",
    "\n",
    "            #Fiducial points when bandpass is updated\n",
    "            self.fiducial_points = find_fiducial_point(self.mwi_y,self.mwi_x,self.test_segments,self.segment_duration,self.mwi_window)\n",
    "            self.max_x, self.max_y, self.peaks_x, self.peaks_y, self.interpolated_peak_x, self.interpolated_peak_y,self.max_ids = interpolation_spline(\n",
    "                self.fiducial_points, self.time, self.values, self.spline, self.res)\n",
    "                    \n",
    "    @property\n",
    "    def mwi_window(self):\n",
    "        return self._mwi_window\n",
    "    \n",
    "    @mwi_window.setter\n",
    "    def mwi_window(self,new_window):\n",
    "        self._mwi_window = new_window\n",
    "        self.mwi_y = dif_eq_window_integration(self.sq_y,self.mwi_window,False)\n",
    "        self.mwi_x = self.der_x\n",
    "        self.fiducial_points = find_fiducial_point(self.mwi_y,self.mwi_x,self.test_segments,self.segment_duration,self.mwi_window)\n",
    "        self.max_x, self.max_y, self.peaks_x, self.peaks_y, self.interpolated_peak_x, self.interpolated_peak_y,self.max_ids = interpolation_spline(\n",
    "                self.fiducial_points, self.time, self.values, self.spline, self.res)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "        #self.prepro_fig = go.Figure()\n",
    "        #self.prepro_fig.add_trace(go.Scatter(line=dict(color='orange'),x = self.prepro_x, y = self.prepro_data),showlegend = show_legend)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c5e4752-816a-486a-b5c4-bd0a7a8afb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the indexes of the intaken data where the fiducial point is\n",
    "def find_fiducial_point(y,x,test_segments,segment_duration,window_length):\n",
    "    fiducial_point = []\n",
    "    peak = 0\n",
    "    found_p = False\n",
    "    mwi_peak = []\n",
    "    look_for_peaks = True\n",
    "    counter = 0\n",
    "    peak_average, first_peaks = first_peaks_height_av(y,test_segments,0.8)\n",
    "\n",
    "    for i in range(1,len(x)-1):\n",
    "        if look_for_peaks:    \n",
    "            f_derivative = (y[i+1]-y[i])/(x[i+1]-x[i])\n",
    "\n",
    "            if found_p:\n",
    "                if f_derivative > 0: \n",
    "                    found_p = False\n",
    "                    if (y[i]<(0.9)*peak) : \n",
    "                        found_p = True\n",
    "                        \n",
    "                if (peak/2>y[i]) :\n",
    "                    fiducial_point = np.append(fiducial_point,i-window_length)\n",
    "                    mwi_peak = np.append(mwi_peak,y[i])\n",
    "                    found_p = False\n",
    "                    peak = 0\n",
    "                    look_for_peaks = False\n",
    "                    if len(fiducial_point)>test_segments:\n",
    "                        peak_average = np.average(mwi_peak)\n",
    "                        \n",
    "            else: \n",
    "                b_derivative = (y[i]-y[i-1])/(x[i]-x[i-1])\n",
    "                if (f_derivative < 0) and (b_derivative > 0) and y[i]>0.5*peak_average: \n",
    "                    peak = y[i]\n",
    "                    found_p = True\n",
    "                    \n",
    "        else: \n",
    "            counter = counter + 1\n",
    "            if counter== 45:\n",
    "                counter = 0 \n",
    "                look_for_peaks = True\n",
    "            \n",
    "\n",
    "    \n",
    "    fiducial_point = np.asarray(fiducial_point, dtype = 'int')\n",
    "\n",
    "    return fiducial_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6997df68-88fe-4c6b-a522-049584b0c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(data, lowcut, highcut, signal_freq, filter_order):\n",
    "    nyquist_freq = 0.5 * signal_freq\n",
    "    low = lowcut / nyquist_freq\n",
    "    high = highcut / nyquist_freq\n",
    "    b, a = butter(filter_order, [low, high], btype=\"band\")\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0bb2149-70c4-4270-8a8f-e21415c3bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#found_peaks needs to be the index of the peaks in the raw_data_values or raw_data_time\n",
    "# This function returns the maximum or the interpolated peaks and the interpolated peak itself\n",
    "def interpolation(found_peaks,raw_data_time, raw_data_values):\n",
    "    \n",
    "    # First identify if we are past the peak or before \n",
    "    amount_of_peaks = len(found_peaks)\n",
    "    corrected_peaks_dict = {}; interpolated_peak_y = {} ;interpolated_peak_x = {};\n",
    "    peaks_x = []; peaks_y = [];\n",
    "\n",
    "    f = np.zeros(3)\n",
    "\n",
    "    for i in range (0,amount_of_peaks): \n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        index = found_peaks[i]\n",
    "        point = raw_data_values[index]\n",
    "        previous_point = raw_data_values[index-1]\n",
    "\n",
    "        if previous_point < point: \n",
    "            while previous_point < point: \n",
    "                index = index + 1\n",
    "                point = raw_data_values[index]\n",
    "                previous_point = raw_data_values[index-1]\n",
    "\n",
    "            # When the while isn't valid the maximum is located at index-1\n",
    "\n",
    "            last_p = index + 2\n",
    "            first_p = index - 3\n",
    "\n",
    "        #point will be the point past the maximum \n",
    "        else: \n",
    "            while previous_point > point: \n",
    "                index = index - 1\n",
    "                point = raw_data_values[index]\n",
    "                previous_point = raw_data_values[index-1]\n",
    "\n",
    "            #when the while finished or isn't valid the maximum is located at index\n",
    "            last_p = index+3\n",
    "            first_p = index-2\n",
    "\n",
    "        # Were staying with the maximum, one forward, and two previous points \n",
    "\n",
    "        r = raw_data_values[[first_p,first_p+2,last_p-1,last_p]]\n",
    "        x = raw_data_time[[first_p,first_p+2,last_p-1,last_p]]\n",
    "\n",
    "        # interpolation \n",
    "\n",
    "        f[0] = (x[2] - x[0]) / (x[1] - x[0])\n",
    "        f[1] = (x[3] - x[3]) / (x[1] - x[0])\n",
    "        f[2] = (((x[3]**2) - (x[0]**2)) - (f[1]*((x[1]**2) - (x[0]**2)))) / (((x[2]**2) - (x[0]**2)) - (f[0]*((x[1]**2) - (x[0]**2))))\n",
    "\n",
    "\n",
    "        d = ( r[3]-r[0] - ( f[1] * ( r[1] - r[0] ) ) - ( ( r[2] - r[0] - (f[0] * (r[1]-r[0]) ) )*f[1]) ) / ((x[3]**3) - (x[0]**3) - ( ( (x[1]**3) - (x[0]**3) ) * f[1]) - (f[2]*( ( (x[2]**3) - (x[0]**3) ) - (f[0] * ( (x[1]**3)-(x[0]**3) )  ) ) ))\n",
    "        c = ( r[2] - r[0] - ( (r[1]-r[0])*f[0]) - ( d * ( (x[2]**3) - (x[0]**3) - (f[0]*( (x[1]**3) - (x[0]**3) ) ) ) ) ) / ( (x[2]**2) - (x[0]**2) - ( f[0] * ( (x[1]**2) - (x[0]**2) )  ) )\n",
    "        b = ( r[1] - r[0] - (d*((x[1]**3) - (x[0]**3))) - (c*((x[1]**2) - (x[0]**2))) ) / (x[1]-x[0])\n",
    "        a = r[0] - (b*x[0]) - (c*(x[0]**2)) - (d*(x[0]**3))\n",
    "\n",
    "        x_interp = np.arange(x[0],x[2]+0.5,0.5)\n",
    "        y = a + (b*x_interp) + (c*(x_interp**2)) + (d*(x_interp**3))\n",
    "        max_index = np.where(max(y)==y)\n",
    "\n",
    "        peaks_x = np.append(peaks_x,x_interp[max_index][0])\n",
    "        peaks_y = np.append(peaks_y, y[max_index][0])\n",
    "        interpolated_peak_x['peak: '+str(i)] = x_interp\n",
    "        interpolated_peak_y['peak: '+str(i)] = y\n",
    "        corrected_peaks_dict['peak: '+str(i) ] = [x_interp[max_index][0],y[max_index][0]]\n",
    "        \n",
    "        \n",
    "    return peaks_x, peaks_y, interpolated_peak_x, interpolated_peak_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da62a572-5345-44cd-96c1-a97274074d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices={}\n",
    "three_spline_m = np.array([[4, 1, 0, 0, 0, 0]])\n",
    "four_spline_m =  np.array([[4, 1, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "two_spline_m =  np.array([[4, 1, 0, 0], [1, 4, 1, 0], [0, 1 , 4 , 1], [0, 0, 1, 4]])\n",
    "\n",
    "new_row_3 = np.array([1, 4, 1, 0, 0, 0,])\n",
    "new_row_4 = np.array([1, 4, 1, 0, 0, 0, 0, 0])\n",
    "\n",
    "for l in range(0,(6-2)):\n",
    "    three_spline_m = np.vstack([three_spline_m, np.roll(new_row_3, l)])\n",
    "three_spline_m = np.vstack([three_spline_m, np.array([[0, 0, 0, 0, 1, 4]])])\n",
    "\n",
    "for u in range(0,(8-2)):\n",
    "    four_spline_m = np.vstack([four_spline_m, np.roll(new_row_4, u)])\n",
    "four_spline_m = np.vstack([four_spline_m, np.array([[0, 0, 0, 0, 0, 0, 1, 4]])])\n",
    "\n",
    "inverse_two_spline_m = np.linalg.inv(two_spline_m); inverse_three_spline_m = np.linalg.inv(three_spline_m); inverse_four_spline_m = np.linalg.inv(four_spline_m)\n",
    "matrices['2_splines'] = inverse_two_spline_m; matrices['3_splines'] = inverse_three_spline_m; matrices['4_splines'] = inverse_four_spline_m;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90006a82-dc58-4ff2-bc21-1519f6d0d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xn = np.arange(400,412 + 1000/5000 ,1000/5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2b6da11-51f4-4a79-9262-de194ae6d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#found_peaks needs to be the index of the peaks in the raw_data_values or raw_data_time\n",
    "# This function returns the maximum or the interpolated peaks and the interpolated peak itself\n",
    "# splines is how many splines you want to carry out around the peak, minumum is 1 for it to have  \n",
    "# one polynomial to the right and one to the left\n",
    "\n",
    "def interpolation_spline(found_peaks, raw_data_time, raw_data_values, splines, resolution):\n",
    "    \n",
    "    res = 1000/resolution\n",
    "    \n",
    "    # First identify if we are past the peak or before \n",
    "    h = 4;\n",
    "    max_indices = []\n",
    "    \n",
    "    #amount of control points\n",
    "    amount_of_peaks = len(found_peaks)\n",
    "    corrected_peaks_dict = {}; interpolated_peak_y = {} ;interpolated_peak_x = {};\n",
    "    peaks_x = []; peaks_y = []; max_x = []; max_y = [];\n",
    "\n",
    "    f = np.zeros(3)\n",
    "    for i in range (0,amount_of_peaks): \n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        index = found_peaks[i]\n",
    "        point = raw_data_values[index]\n",
    "        previous_point = raw_data_values[index-1]\n",
    "\n",
    "        #Finding the maximum\n",
    "        \n",
    "        if previous_point < point: \n",
    "            while previous_point < point: \n",
    "                index = index + 1\n",
    "                point = raw_data_values[index]\n",
    "                previous_point = raw_data_values[index-1]\n",
    "\n",
    "            # When the while isn't valid the maximum is located at index-1\n",
    "            max_idx = index - 1\n",
    "\n",
    "\n",
    "        #point will be the point past the maximum \n",
    "        \n",
    "        else: \n",
    "            while previous_point > point: \n",
    "                index = index - 1\n",
    "                point = raw_data_values[index]\n",
    "                previous_point = raw_data_values[index-1]\n",
    "\n",
    "            #when the while finished or isn't valid the maximum is located at index\n",
    "            \n",
    "            max_idx = index\n",
    "        \n",
    "        max_indices = np.append(max_indices,max_idx)\n",
    "        last_p = max_idx + splines + 2\n",
    "        first_p = max_idx - (splines + 1)\n",
    "\n",
    "        # r and x include from x_{-1} up to x_{n+3}\n",
    "        r = raw_data_values[first_p:last_p+1]\n",
    "        x = raw_data_time[first_p:last_p+1]\n",
    "\n",
    "        # interpolation \n",
    "        \n",
    "        \n",
    "        #M_(-1)\n",
    "        m_1 =  (r[2] - (2*r[1]) + r[0]) / (h**2)\n",
    "        #M_(n+1)\n",
    "        m_n_1 =  (r[-1] - (2*r[-2]) + r[-3]) /(h**2)\n",
    "        ys =[]\n",
    "        ys = np.append( ys, ( (6/(h**2)) * (r[0] - (2*r[1]) + r[2]) ) + m_1 )\n",
    "        actual_cp = splines*2\n",
    "        \n",
    "        for n in range(1,(actual_cp-2)+1):\n",
    "            ys = np.append( ys, (6/(h**2)) * (r[n] - (2*r[n+1]) + r[n+2]) )\n",
    "        \n",
    "        ys = np.append( ys, (6/(h**2)) * (r[actual_cp-1] - (2*r[actual_cp]) + r[actual_cp+1]) - m_n_1)\n",
    "        \n",
    "        \n",
    "        # Now the define inverted matrices\n",
    "        \n",
    "        inverted_matrix = matrices[str(splines)+'_splines']\n",
    "        \n",
    "        \n",
    "        Ms = np.matmul(inverted_matrix, ys)\n",
    "        b = Ms/2\n",
    "        #We are appending m_n_1 which is the variable for M_{n+1} because we need it to calculate the c's\n",
    "        Ms = np.append(Ms,m_n_1)\n",
    "        a = (Ms[1:actual_cp+1] - Ms[0:actual_cp])/(6*h)\n",
    "        c = ((r[2:(actual_cp+1)+1] - r[1:actual_cp + 1])/h) - ((h/6)*(Ms[1:actual_cp+1]+(2*Ms[0:actual_cp])))\n",
    "        d = r[1:actual_cp +1] \n",
    "        # Remember the plus one is to include the last element which is the actual_cp index other wise python doesn't touch the last elemment\n",
    "        \n",
    "        interpolated_y = []\n",
    "        \n",
    "        #Piecewise polynomials\n",
    "        for k in range (1,(splines*2)): \n",
    "            x_int = np.arange(x[k],x[k+1],res)\n",
    "            y_int = (a[k-1]*((x_int - x[k])**3)) + (b[k-1]*((x_int - x[k])**2)) + (c[k-1]*(x_int-x[k])) + d[k-1]\n",
    "            interpolated_y = np.concatenate((interpolated_y,y_int))\n",
    "            \n",
    "        # We delete every last point so that it doesn't repeat with the first of the next piecewise polynomial\n",
    "        # So this last block of code is to include the last interpolation with the last point\n",
    "        l_cp = splines*2\n",
    "        x_int = np.arange(x[l_cp],x[l_cp+1]+res,res)\n",
    "        y_int = (a[l_cp-1]*((x_int - x[l_cp])**3)) + (b[l_cp-1]*((x_int - x[l_cp])**2)) + (c[l_cp-1]*(x_int-x[l_cp])) + d[l_cp-1]\n",
    "        interpolated_y = np.concatenate((interpolated_y, y_int))\n",
    "        interpolated_x = np.arange(x[1],x[(splines*2)+1]+res ,res)\n",
    "        \n",
    "        interpolated_peak_y['peak: '+str(i)] = interpolated_y; interpolated_peak_x['peak: '+str(i)] = interpolated_x; \n",
    "        \n",
    "        # getting the maximum collecting  vallues\n",
    "        max_index = np.where(max(interpolated_y)==interpolated_y)\n",
    "\n",
    "        peaks_x = np.append(peaks_x,interpolated_x[max_index][0])\n",
    "        peaks_y = np.append(peaks_y, interpolated_y[max_index][0])\n",
    "        \n",
    "        #Analytical Solution\n",
    "        \n",
    "        roots_1 = []; roots_2 = []; s_eval_roots = []; s2_eval_roots =[]\n",
    "        aj = a[splines-1:splines+1]; bj = b[splines-1:splines+1]; cj = c[splines-1:splines+1]; dj = d[splines-1:splines+1]; xj = x[splines:splines+2]\n",
    "        \n",
    "        #Coefficients of general formula 1 is for the first piecewise poolynomial and 2 is for the second piecewise polynomial\n",
    "        g_a1 = 3*aj[0]; g_b1 = (2*bj[0] - (2*xj[0]*3*aj[0])); g_c1 = (3*aj[0]*(xj[0]**2)) - (2*bj[0]*xj[0]) + cj[0]\n",
    "        g_a2 = 3*aj[1]; g_b2 = (2*bj[1] - (2*xj[1]*3*aj[1])); g_c2 = (3*aj[1]*(xj[1]**2)) - (2*bj[1]*xj[1]) + cj[1]\n",
    "\n",
    "        roots_1 = np.append(roots_1, (- g_b1 + np.sqrt((g_b1**2) - (4 * g_a1 * g_c1 )))/(2*g_a1) );\n",
    "        roots_1 = np.append(roots_1, (- g_b1 - np.sqrt((g_b1**2) - (4 * g_a1 * g_c1 )))/(2*g_a1) );\n",
    "        s_eval_roots = np.append(s_eval_roots, aj[0]*((roots_1[0]-xj[0])**3) + bj[0]*((roots_1[0]-xj[0])**2) + cj[0]*(roots_1[0]-xj[0]) + dj[0])\n",
    "        s_eval_roots = np.append(s_eval_roots, aj[0]*((roots_1[1]-xj[0])**3) + bj[0]*((roots_1[1]-xj[0])**2) + cj[0]*(roots_1[1]-xj[0]) + dj[0])\n",
    "        \n",
    "        if roots_1[np.argmax(s_eval_roots)] <=xj[1]:\n",
    "            max_x = np.append(max_x, roots_1[np.argmax(s_eval_roots)])\n",
    "            max_y = np.append(max_y, s_eval_roots[np.argmax(s_eval_roots)])\n",
    "            \n",
    "        else: \n",
    "            roots_2 = np.append(roots_2, (- g_b2 + np.sqrt((g_b2**2) - (4 * g_a2 * g_c2 )))/(2*g_a2) );\n",
    "            roots_2 = np.append(roots_2, (- g_b2 - np.sqrt((g_b2**2) - (4 * g_a2 * g_c2 )))/(2*g_a2) );\n",
    "            s2_eval_roots = np.append(s2_eval_roots, aj[1]*((roots_2[0]-xj[1])**3) + bj[1]*((roots_2[0]-xj[1])**2) + cj[1]*(roots_2[0]-xj[1]) + dj[1])\n",
    "            s2_eval_roots = np.append(s2_eval_roots, aj[1]*((roots_2[1]-xj[1])**3) + bj[1]*((roots_2[1]-xj[1])**2) + cj[1]*(roots_2[1]-xj[1]) + dj[1])\n",
    "            max_x = np.append(max_x, roots_2[np.argmax(s2_eval_roots)])\n",
    "            max_y = np.append(max_y, s2_eval_roots[np.argmax(s2_eval_roots)])\n",
    "            \n",
    "    max_indices = np.asarray(max_indices, dtype = 'int')\n",
    "        \n",
    "    #max_x and max_y is for analytical solutions, peaks_x and peaks_y is for maximum by collecting data, and interpolated_peak_x/y is the actual interpolation\n",
    "    return max_x, max_y, peaks_x, peaks_y, interpolated_peak_x, interpolated_peak_y,max_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e33b2a8-30a7-4d87-9b8d-1b81172a3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes two arrays of peaks, where each value in the array is the index of the peak, and finds \n",
    "# if any of them have missing peaks compared to the other array\n",
    "# Index comparing is when you are comparing the index of two different arrays but you might want to compare actual elements\n",
    "\n",
    "def find_missing_peaks(a_peaks, b_peaks, index_comparing):\n",
    "    missing_b_peaks = []; missing_a_peaks = []\n",
    "\n",
    "    if len(a_peaks)<len(b_peaks):\n",
    "        #a_peaks is the shortest in length\n",
    "        a_peaks=np.append(a_peaks,np.zeros(len(b_peaks) - len(a_peaks)))        \n",
    "\n",
    "    else: \n",
    "        #b_peaks is the shortest in length\n",
    "        b_peaks=np.append(b_peaks,np.zeros(len(a_peaks) - len(b_peaks)))\n",
    "\n",
    "        \n",
    "    difference = abs(a_peaks - b_peaks) #Subtraction order don't matter\n",
    "\n",
    "    a_insertions = 0; b_insertions = 0;\n",
    "\n",
    "    while (difference>70).any():\n",
    "        \n",
    "\n",
    "        #Here we first find in what index is where a mark is misssing\n",
    "\n",
    "        index = np.min(np.where(difference>70))\n",
    "        #This defines if either the short pivot or the long pivot array is missing a mark on a peak\n",
    "        \n",
    "        # If True then there's a missing peak in the b pivot\n",
    "\n",
    "        if (a_peaks[index] - b_peaks[index]) < 0:\n",
    "            \n",
    "\n",
    "            #This is for handling the last zeros. When you arrive to the ending zeros it means that you will not do \n",
    "            # more insertions but rather replace the zeros for the peaks\n",
    "            \n",
    "            if a_peaks[index] == 0:\n",
    "                a_peaks = np.insert(a_peaks,index,b_peaks[index])\n",
    "                a_peaks = np.delete(a_peaks,-1)\n",
    "                missing_a_peaks = np.append(missing_a_peaks,index-b_insertions)\n",
    "\n",
    "            else:\n",
    "                missing_b_peaks = np.append(missing_b_peaks,index-a_insertions)\n",
    "                b_peaks = np.insert(b_peaks,index,a_peaks[index])\n",
    "                b_insertions = b_insertions + 1\n",
    "                \n",
    "                \n",
    "                # If last element is zero you can erease last element, otherwise append a 0 to the other array. \n",
    "                if b_peaks[-1] == 0 :\n",
    "                    b_peaks = np.delete(b_peaks,-1)\n",
    "                else:\n",
    "                    a_peaks =  np.append(a_peaks,0)\n",
    "\n",
    "        #There is a missing peak in the a_peaks\n",
    "        else:\n",
    "            \n",
    "           #This is for handling the last zeros. \n",
    "            if b_peaks[index] == 0:\n",
    "\n",
    "                b_peaks = np.insert(b_peaks,index,a_peaks[index])\n",
    "                b_peaks = np.delete(b_peaks,-1)\n",
    "                missing_b_peaks = np.append(missing_b_peaks,index-a_insertions)\n",
    "\n",
    "\n",
    "            else:\n",
    "                missing_a_peaks = np.append(missing_a_peaks,index - b_insertions)\n",
    "                a_peaks = np.insert(a_peaks,index,b_peaks[index])\n",
    "                a_insertions = a_insertions + 1\n",
    "                \n",
    "                if a_peaks[-1] == 0 :\n",
    "                    a_peaks = np.delete(a_peaks,-1)\n",
    "                else:\n",
    "                    b_peaks =  np.append(b_peaks,0)\n",
    "                \n",
    "        difference = abs(a_peaks - b_peaks)\n",
    "    \n",
    "   \n",
    "    if index_comparing: \n",
    "        missing_a_peaks = np.asarray(missing_a_peaks, dtype = 'int'); missing_b_peaks = np.asarray(missing_b_peaks, dtype = 'int')\n",
    "    else: \n",
    "        missing_a_peaks = np.asarray(missing_a_peaks, dtype = 'float'); missing_b_peaks = np.asarray(missing_b_peaks, dtype = 'float')\n",
    "        \n",
    "        \n",
    "    # The outcome are the indices that are missing in the other array. For example if a is missing the index N in the b array \n",
    "    # then the index N is stored in missing_a_peaks\n",
    "    # missing_a_peaks are indices of the b array, with elements that a doesn't contain\n",
    "    return missing_a_peaks, missing_b_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e157bb0-ab19-4c42-8a9e-39fe29e93c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This moving average function which serves as a difference equation is a central moving average which means that you are \n",
    "# averaging a point taking into account both sides neighbors. So at last you will need to cut either (window_length-1)/2)\n",
    "# or (window_length-2)/2) depending wether you are having an even or odd number in the window length\n",
    "\n",
    "def moving_average(window_length, y):\n",
    "    \n",
    "    x=y\n",
    "    M = window_length - 1\n",
    "    z = np.zeros(M+1)\n",
    "    x_modified = np.concatenate((z,x[0:len(x) - (M+1)]))\n",
    "    y1= (1/(M+1)) * (x[0]-x_modified[0])\n",
    "    yn = [y1]\n",
    "\n",
    "    for n in range (1,len(x)):\n",
    "        y_value = ((1/(M+1)) * (x[n]-x_modified[n])) + yn[n-1]\n",
    "        yn = np.append(yn,y_value)\n",
    "\n",
    "    if window_length% 2 == 1: \n",
    "        cutting_points = int((window_length-1)/2) \n",
    "        yn = yn[cutting_points:len(yn)]\n",
    "        \n",
    "    else: \n",
    "        cutting_points = int((window_length-2)/2)\n",
    "        yn = yn[cutting_points:len(yn)]\n",
    "        \n",
    "    return yn, cutting_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37400d19-429b-4b2b-9c46-3a90c5f0cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_filter(window_length,signal):\n",
    "\n",
    "    m2 = window_length - 1\n",
    "\n",
    "    u = np.heaviside(np.arange(0,len(signal)),1)\n",
    "    u_sub = np.concatenate([np.zeros(window_length),np.heaviside(np.arange(0,len(signal)-(window_length)),1)])\n",
    "    h_n_s= (1/(window_length)) * (u - u_sub)\n",
    "\n",
    "    mov_av_s = np.convolve (h_n_s,signal)\n",
    "    mov_av_s_sh = mov_av_s[int(m2/2):len(signal)+int(m2/2)]\n",
    "    \n",
    "    return mov_av_s_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44ff6a16-3f32-4c22-8675-1d0336cc9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif_eq_window_integration(function,window_length,divide):\n",
    "    \n",
    "    pivot_function = function\n",
    "    \n",
    "    for i in range(1,window_length):\n",
    "        pivot_function = np.insert(pivot_function,0,0)\n",
    "        pivot_function = np.delete(pivot_function,len(pivot_function)-1)\n",
    "        #print(pivot_function)\n",
    "        function = function + pivot_function\n",
    "\n",
    "        if divide: \n",
    "            result = (1/N)*function\n",
    "        else: \n",
    "            result = function\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4b97028-ba88-4cca-a1ca-e9b0b4d5f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_peaks_height_av(y,segments,fraction_of_a_second):\n",
    "\n",
    "    s = 0\n",
    "    max_values = []\n",
    "    \n",
    "    f = 250*fraction_of_a_second\n",
    "\n",
    "    while s < segments: \n",
    "\n",
    "        max_values = np.append(max_values,np.max(y[int(f*s):int(f*(s+1))]))\n",
    "        s = s+1\n",
    "\n",
    "    average_peak_h = np.sum(max_values)/segments\n",
    "    \n",
    "    return average_peak_h, max_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ba35c-8863-41a4-885c-61f31f7afa7d",
   "metadata": {},
   "source": [
    "## Cluster Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fad5b3dd-87de-419d-8c94-16395dcb7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sample = raw_sample_(1,30,3,0,1)\n",
    "fig_raw = cluster_sample.fig\n",
    "fig_raw.update_layout(xaxis = dict(tickmode = 'array', tickvals = cluster_sample.ticks_values, ticktext = cluster_sample.time_format_axis),autosize=False,width=1400,\n",
    "                  height=600,showlegend=False,title=\"Raw Data \", xaxis_title=\"Time\", yaxis_title= 'Signal', \n",
    "                  font=dict(family=\"Avenir\",size=14,color=\"Black\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63f0a8-ad47-4cdd-a3e4-5562c446829a",
   "metadata": {},
   "source": [
    "## Pan Tompkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0d82f-6663-4921-8a4c-42037f6dddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample One: sample_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6095732-f898-405b-8694-1443a5e1d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_min_t = 1; min_sec_t = 30; max_min_t= 3; max_sec_t = 0; seconds_per_label_t = 1; inverted_t = False;   bs_r_window_t = 6; \n",
    "band_start_t = 3; band_stop_t = 100; fs_t= 250;  trans_width_t = 2.88; numtaps_t = 152; show_legend_t = False; mwi_window_t = 25 ;\n",
    "test_segments_t =3;            spline_t = 4;       res_t = 5000;\n",
    "\n",
    "sample_test = sample_all_included(min_min_t,min_sec_t,max_min_t,max_sec_t,seconds_per_label_t,inverted_t, bs_r_window_t,band_start_t,band_stop_t,fs_t,\n",
    "                       trans_width_t,numtaps_t, mwi_window_t, test_segments_t, spline_t,res_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "680a65c5-4789-4db5-bad0-0b3aaac8b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Two: sample_test_dif_wl    this sample will have different window length for the mean average subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce32dae9-39c2-4edb-b832-bdda0582d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_min_t = 1; min_sec_t = 30; max_min_t= 3; max_sec_t = 0; seconds_per_label_t = 1; inverted_t = False;   bs_r_window_t = 10; \n",
    "band_start_t = 3; band_stop_t = 100; fs_t= 250;  trans_width_t = 2.88; numtaps_t = 152; show_legend_t = False; mwi_window_t = 25 ;\n",
    "test_segments_t =3;            spline_t = 4;       res_t = 5000;\n",
    "\n",
    "sample_test_dif_wl = sample_all_included(min_min_t,min_sec_t,max_min_t,max_sec_t,seconds_per_label_t,inverted_t, bs_r_window_t,band_start_t,band_stop_t,fs_t,\n",
    "                       trans_width_t,numtaps_t, mwi_window_t, test_segments_t, spline_t,res_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b1549fd-859a-4323-bee2-aba19c40f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_raw = make_subplots(rows=3, cols=1, subplot_titles=(\"Peaks \", 'Mean Subtraction', 'Moving Window Integration'))\n",
    "\n",
    "fig_raw.add_trace(go.Scatter(line = dict(color = 'blue'), x = sample_test.time,y = sample_test.values,showlegend=False),row=1, col=1)\n",
    "fig_raw.add_trace(go.Scatter(mode = 'markers', x = sample_test.max_x,y = sample_test.max_y,showlegend=False),row=1, col=1)\n",
    "fig_raw.add_trace(go.Scatter(x = sample_test.prepro_x,y = sample_test.prepro_data,showlegend=False),row=2,col=1)\n",
    "fig_raw.add_trace(go.Scatter(line=dict(color='#0a4345'),x = sample_test.mwi_x,y = sample_test.mwi_y,showlegend=False),row=3,col=1)\n",
    "fig_raw.add_trace(go.Scatter(marker=dict(color='red'),mode ='markers',x = sample_test.mwi_x[sample_test.fiducial_points],y = sample_test.mwi_y[sample_test.fiducial_points],\n",
    "                              showlegend=False),row=3,col=1)\n",
    "\n",
    "\n",
    "\n",
    "fig_raw.update_layout(xaxis = dict(tickmode = 'array', tickvals = sample_test.ticks_values, ticktext = sample_test.time_format_axis),\n",
    "                    xaxis2 = dict(tickmode = 'array', tickvals = sample_test.ticks_values, ticktext = sample_test.time_format_axis),\n",
    "                    xaxis3 = dict(tickmode = 'array', tickvals = sample_test.ticks_values, ticktext = sample_test.time_format_axis))\n",
    "\n",
    "fig_raw.update_layout(xaxis_range=[79000,85000], xaxis2_range=[79000,85000],xaxis3_range=[79000,85000],yaxis_range=[-1.5,1.3],\n",
    "                       yaxis2_range=[-0.4,0.4], yaxis3_range=[-0.001,0.05])\n",
    "fig_raw.update_layout(height=1200,width = 1200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d285c0e-283e-4f9a-8910-ba32f27c0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig_raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d762ff-4e8c-4c71-8e3b-39c43950eb30",
   "metadata": {},
   "source": [
    "## Mising Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78881a8f-7283-41b7-83f5-8fe1c97e8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pt_p, missing_cluster_p = find_missing_peaks(sample_test.max_x, cluster_sample.pks_time, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b12bdaa8-25bc-44f6-93e6-a94967e67606",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_c_vs_s =go.Figure()\n",
    "\n",
    "fig_c_vs_s.add_trace(go.Scatter(line= dict(color='blue'),x=cluster_sample.time, y=cluster_sample.values,name='Raw Data',showlegend=True))\n",
    "    \n",
    "\n",
    "fig_c_vs_s.add_trace(go.Scatter(mode='markers',marker=dict(size=12),x=cluster_sample.pks_time, y= cluster_sample.pks_values, name = 'Cluster', \n",
    "                           showlegend=True,))\n",
    "\n",
    "fig_c_vs_s.add_trace(go.Scatter(mode='markers',marker=dict(size=10,color='#21B626'), x = sample_test.max_x, \n",
    "                          y = sample_test.max_y, name = 'Current Version', showlegend=True))\n",
    "\n",
    "fig_c_vs_s.add_trace(go.Scatter(mode='markers',marker=dict(size=7,color='orange'), x = sample_test.max_x[missing_cluster_p], \n",
    "                           y= sample_test.max_y[missing_cluster_p], name = 'Missing Cluster Peaks', showlegend=True))\n",
    "\n",
    "fig_c_vs_s.add_trace(go.Scatter(mode='markers',marker=dict(size=7,color='#00FCF5'),x = cluster_sample.pks_time[missing_pt_p] , y= cluster_sample.pks_values[missing_pt_p], \n",
    "                          name = 'Missing Current Version Peaks', showlegend=True))\n",
    "\n",
    "\n",
    "fig_c_vs_s.update_layout(xaxis = dict(tickmode = 'array', tickvals = cluster_sample.ticks_values, ticktext = cluster_sample.time_format_axis),autosize=False,width=1200,height=630,\n",
    "                    title='Missing Cluster Peaks: '+str(len(missing_cluster_p))+'           Missing Current Version Peaks: '+str(len(missing_pt_p)) + \n",
    "                   '            MAW: '+str(sample_test.bs_r_window) + '         Band: '+str(sample_test.band_start)+' - '+str(sample_test.band_stop));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd54aec7-f562-494f-bdcb-c5b635d1017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig_c_vs_s.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248db9c-6eb9-4cf8-b4ce-664d094b0c10",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18743fb4-d683-4530-bbe1-35e7b607fcf5",
   "metadata": {},
   "source": [
    "## Comparing different window lengths against 6 window length in mean average subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb48c9b8-7b7f-4fb8-bfe8-41d8c11d2453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16800/270883639.py:124: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n",
      "/tmp/ipykernel_16800/270883639.py:125: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Visually missing peaks\n",
    "\n",
    "fig_ms_p = go.Figure()\n",
    "missing_6_dict ={};  missing_dif_wl_dict={};\n",
    "spline=4;\n",
    "res = 5000\n",
    "lengths = np.arange(7,20,1)\n",
    "\n",
    "\n",
    "for N in lengths: \n",
    "    sample_test_dif_wl.bs_r_window = N\n",
    "    missing_6_p, missing_dif_wl_p = find_missing_peaks(sample_test.max_x, sample_test_dif_wl.max_x, True)\n",
    "    missing_6_dict['window_length_'+str(N)] = missing_6_p\n",
    "    missing_dif_wl_dict['window_length_'+str(N)] = missing_dif_wl_p\n",
    "    \n",
    "    fig_ms_p.add_trace(go.Scatter(line= dict(color='blue'),x=sample_test.time, y=sample_test.values,name='Raw Data',showlegend=True, visible = False))\n",
    "    \n",
    "\n",
    "    fig_ms_p.add_trace(go.Scatter(mode='markers',marker=dict(size=12),x=sample_test.max_x, y= sample_test.max_y, name = '6 WL Peaks', \n",
    "                               showlegend=True, visible = False))\n",
    "    fig_ms_p.add_trace(go.Scatter(mode='markers',marker=dict(size=10,color='#21B626'), x = sample_test_dif_wl.max_x, y = sample_test_dif_wl.max_y,\n",
    "                               name = str(N)+' WL Peaks',showlegend=True, \n",
    "                              visible = False))\n",
    "    fig_ms_p.add_trace(go.Scatter(mode='markers',marker=dict(size=7,color='orange'), x = sample_test_dif_wl.max_x[missing_6_p], \n",
    "                               y= sample_test_dif_wl.max_y[missing_6_p], name = '6 WL Missing Peaks', showlegend=True, visible = False))\n",
    "    \n",
    "    fig_ms_p.add_trace(go.Scatter(mode='markers',marker=dict(size=7,color='#00FCF5'),x = sample_test.max_x[missing_dif_wl_p] , y= sample_test.max_y[missing_dif_wl_p], \n",
    "                              name = str(N)+' WL Missing  Peaks', showlegend=True, visible = False))\n",
    "\n",
    "fig_ms_p.data[5].visible = True\n",
    "\n",
    "steps = []\n",
    "start = 0\n",
    "for i in range(0,len(lengths)):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig_ms_p.data)},\n",
    "              {\"title\": \"Peaks identified with window length \"+str(lengths[i])+ '   Amount of 6 WL Missing Peaks: '+\n",
    "               str(len(missing_6_dict['window_length_'+str(lengths[i])])) +'    Amount of ' + str(lengths[i]) + ' WL Missing Peaks: '+ \n",
    "               str(len(missing_dif_wl_dict['window_length_'+str(lengths[i])]))}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][start] = True  # Toggle i'th trace to \"visible\"\n",
    "    step[\"args\"][0][\"visible\"][start+1] = True\n",
    "    step[\"args\"][0][\"visible\"][start+2] = True\n",
    "    step[\"args\"][0][\"visible\"][start+3] = True\n",
    "    step[\"args\"][0][\"visible\"][start+4] = True\n",
    "    \n",
    "    start = start+5\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active=5, currentvalue={\"prefix\": \"n: \"}, pad={\"t\": 50}, steps=steps)]\n",
    "fig_ms_p.update_layout(sliders=sliders);\n",
    "fig_ms_p.update_layout(xaxis_range=[79000,85000]);\n",
    "fig_ms_p.update_layout(xaxis = dict(tickmode = 'array', tickvals = cluster_sample.ticks_values, ticktext = cluster_sample.time_format_axis),autosize=False,width=1200,height=600); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fdf083f-7b4a-4624-8ed6-265d753dadeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fig_ms_p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d0fc7-7cba-4e64-97a8-f5215d200dcd",
   "metadata": {},
   "source": [
    "## Pre Processed data with different window lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edef054b-324a-474e-9236-31e38f23d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16800/270883639.py:124: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n",
      "/tmp/ipykernel_16800/270883639.py:125: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fig_prepro = go.Figure()\n",
    "missing_6_dict ={};  missing_dif_wl_dict={};\n",
    "spline=4;\n",
    "res = 5000\n",
    "lengths = np.arange(7,20,1)\n",
    "\n",
    "\n",
    "for N in lengths: \n",
    "    sample_test_dif_wl.bs_r_window = N\n",
    "    \n",
    "    fig_prepro.add_trace(go.Scatter(x = sample_test_dif_wl.prepro_x , \n",
    "                              y= sample_test_dif_wl.prepro_data, name = str(N)+' WL Prepro Data', showlegend=True, visible = False))\n",
    "\n",
    "fig_prepro.data[3].visible = True\n",
    "\n",
    "steps = []\n",
    "start = 0\n",
    "for i in range(0,len(lengths)):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig_prepro.data)},\n",
    "              {\"title\": 'Preprocessed data, window length: '+str(lengths[i])}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active=5, currentvalue={\"prefix\": \"n: \"}, pad={\"t\": 50}, steps=steps)]\n",
    "fig_prepro.update_layout(sliders=sliders);\n",
    "fig_prepro.update_layout(xaxis = dict(tickmode = 'array', tickvals = cluster_sample.ticks_values, ticktext = cluster_sample.time_format_axis),autosize=False,width=1200,height=600); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "056897b3-9adc-441d-a568-831d548176fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig_prepro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7ea9a1-1eaa-4855-8664-d6ce94bf7be6",
   "metadata": {},
   "source": [
    "## Bandpass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d7dc82e-2341-4119-85c3-173fbea3c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16800/270883639.py:124: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n",
      "/tmp/ipykernel_16800/270883639.py:125: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "figbp = go.Figure()\n",
    "missing_6_dict ={};  missing_dif_wl_dict={};\n",
    "spline=4;\n",
    "res = 5000\n",
    "lengths = np.arange(7,20,1)\n",
    "\n",
    "\n",
    "for N in lengths: \n",
    "    sample_test_dif_wl.bs_r_window = N\n",
    "    \n",
    "    figbp.add_trace(go.Scatter(x = sample_test_dif_wl.bp_x , \n",
    "                              y= sample_test_dif_wl.bp_y, name = str(N)+' WL in MS, Bandpass', showlegend=True, visible = False))\n",
    "\n",
    "figbp.data[3].visible = True\n",
    "\n",
    "steps = []\n",
    "start = 0\n",
    "for i in range(0,len(lengths)):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(figbp.data)},\n",
    "              {\"title\": 'Bandpass , window length in mean subtraction: '+str(lengths[i])}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active=5, currentvalue={\"prefix\": \"n: \"}, pad={\"t\": 50}, steps=steps)]\n",
    "figbp.update_layout(sliders=sliders);\n",
    "figbp.update_layout(xaxis = dict(tickmode = 'array', tickvals = cluster_sample.ticks_values, ticktext = cluster_sample.time_format_axis),autosize=False,width=1200,height=600); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c39c296c-7d4b-4615-84c5-93ec43a3ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figbp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001ed80-19d1-4388-bc4b-ac7ffe04d12d",
   "metadata": {},
   "source": [
    "## Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08499230-dddf-4366-bc1e-0091ba68d1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16800/270883639.py:124: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n",
      "/tmp/ipykernel_16800/270883639.py:125: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fig_der = go.Figure()\n",
    "missing_6_dict ={};\n",
    "spline=4;\n",
    "res = 5000\n",
    "lengths = np.arange(7,20,1)\n",
    "\n",
    "\n",
    "for N in lengths: \n",
    "    sample_test_dif_wl.bs_r_window = N\n",
    "    \n",
    "    fig_der.add_trace(go.Scatter(x = sample_test_dif_wl.der_x , \n",
    "                              y= sample_test_dif_wl.der_y, name = str(N)+' WL in MS, Derivative', showlegend=True, visible = False))\n",
    "\n",
    "fig_der.data[3].visible = True\n",
    "\n",
    "steps = []\n",
    "start = 0\n",
    "for i in range(0,len(lengths)):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig_der.data)},\n",
    "              {\"title\": 'Derivative , window length in mean subtraction: '+str(lengths[i])}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active=5, currentvalue={\"prefix\": \"n: \"}, pad={\"t\": 50}, steps=steps)]\n",
    "fig_der.update_layout(sliders=sliders);\n",
    "fig_der.update_layout(xaxis = dict(tickmode = 'array', tickvals = sample_test.ticks_values, ticktext = sample_test.time_format_axis),autosize=False,width=1200,height=600); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59ab676f-8a4f-4312-9c8f-54a2827600d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig_der.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054c4b5-6ea7-45e4-82d7-fb716d8defe2",
   "metadata": {},
   "source": [
    "## Moving Window Integration + Fiducial Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e316c10-db67-4b40-9201-cadd24ada458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16800/270883639.py:124: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n",
      "/tmp/ipykernel_16800/270883639.py:125: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mwi_fp = go.Figure()\n",
    "missing_6_dict ={};  missing_dif_wl_dict={};\n",
    "spline=4;\n",
    "res = 5000\n",
    "lengths = np.arange(7,20,1)\n",
    "\n",
    "\n",
    "for N in lengths: \n",
    "    sample_test_dif_wl.bs_r_window = N\n",
    "    \n",
    "    mwi_fp.add_trace(go.Scatter(x = sample_test_dif_wl.mwi_x , \n",
    "                              y= sample_test_dif_wl.mwi_y, name = str(N)+' Bandpass WL: '+str(N), showlegend=True, visible = False))\n",
    "    mwi_fp.add_trace(go.Scatter(mode = 'markers', x = sample_test_dif_wl.mwi_x[sample_test_dif_wl.fiducial_points] , \n",
    "                              y= sample_test_dif_wl.mwi_y[sample_test_dif_wl.fiducial_points], name = str(N)+' Bandpass WL: '+str(N), \n",
    "                              showlegend=True, visible = False))\n",
    "\n",
    "mwi_fp.data[3].visible = True\n",
    "\n",
    "steps = []\n",
    "start = 0\n",
    "for i in range(0,len(lengths)):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(mwi_fp.data)},\n",
    "              {\"title\": 'Bandpass data, window length: '+str(lengths[i])}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][start] = True  # Toggle i'th trace to \"visible\"\n",
    "    step[\"args\"][0][\"visible\"][start + 1] = True  # Toggle i'th trace to \"visible\"\n",
    "    start = start + 2\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active=5, currentvalue={\"prefix\": \"n: \"}, pad={\"t\": 50}, steps=steps)]\n",
    "mwi_fp.update_layout(sliders=sliders);\n",
    "mwi_fp.update_layout(xaxis = dict(tickmode = 'array', tickvals = cluster_sample.ticks_values, ticktext = cluster_sample.time_format_axis),autosize=False,width=1200,height=600); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88e49217-277c-458b-96cf-2e3fe4b48f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mwi_fp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5905912f-69cc-4886-a3ad-b3cfe5dec5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_min_t = 1; min_sec_t = 30; max_min_t= 3; max_sec_t = 0; seconds_per_label_t = 1; inverted_t = False;   bs_r_window_t = 15; \n",
    "band_start_t = 3; band_stop_t = 100; fs_t= 250;  trans_width_t = 2.88; numtaps_t = 152; show_legend_t = False; mwi_window_t = 25 ;\n",
    "test_segments_t =3;            spline_t = 4;       res_t = 5000;\n",
    "\n",
    "recreated_cluster = sample_all_included(min_min_t,min_sec_t,max_min_t,max_sec_t,seconds_per_label_t,inverted_t, bs_r_window_t,band_start_t,band_stop_t,fs_t,\n",
    "                       trans_width_t,numtaps_t, mwi_window_t, test_segments_t, spline_t,res_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd0721-90cc-4d6d-a71b-ca8c3adf40a3",
   "metadata": {},
   "source": [
    "### Recreating cluster sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fa8c2837-db4b-4e79-b702-8b0c85c6d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_prepro = make_subplots(rows=2, cols=1, subplot_titles=(\"Cluster \", 'MWI: '+str(sample_test.bs_r_window)))\n",
    "comp_prepro.add_trace(go.Scatter(x = recreated_cluster.prepro_x,y = recreated_cluster.prepro_data,showlegend=False),row=1, col=1)\n",
    "comp_prepro.add_trace(go.Scatter(x = sample_test.prepro_x,y = sample_test.prepro_data,showlegend=False),row=2, col=1)\n",
    "\n",
    "comp_prepro.update_layout(xaxis = dict(tickmode = 'array', tickvals = sample_test.ticks_values, ticktext = sample_test.time_format_axis),\n",
    "                          xaxis2 = dict(tickmode = 'array', tickvals = recreated_cluster.ticks_values, ticktext = recreated_cluster.time_format_axis))\n",
    "comp_prepro.update_layout(yaxis_range=[-1.2,1.2],yaxis2_range=[-0.45,0.4])\n",
    "comp_prepro.update_layout(height=900,width = 1200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "619eb581-34c1-4837-b460-638d5a21bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test.band=[4,100]\n",
    "missing_pt_p, missing_cluster_p = find_missing_peaks(sample_test.max_x, cluster_sample.pks_time, True)\n",
    "comp_bandpass = make_subplots(rows=3, cols=1, subplot_titles=('Peaks','MWI: '+str(sample_test.bs_r_window) , 'Cluster'))\n",
    "comp_bandpass.add_trace(go.Scatter(x=cluster_sample.time, y= cluster_sample.values, name = 'Cluster', showlegend=True,),row = 1, col=1)\n",
    "comp_bandpass.add_trace(go.Scatter(mode='markers',marker=dict(size=12),x=cluster_sample.pks_time, y= cluster_sample.pks_values, name = 'Cluster', \n",
    "                           showlegend=True,),row = 1, col=1)\n",
    "\n",
    "comp_bandpass.add_trace(go.Scatter(mode='markers',marker=dict(size=10,color='#21B626'), x = sample_test.max_x, \n",
    "                          y = sample_test.max_y, name = 'Current Version', showlegend=True),row = 1, col=1)\n",
    "\n",
    "comp_bandpass.add_trace(go.Scatter(mode='markers',marker=dict(size=7,color='orange'), x = sample_test.max_x[missing_cluster_p], \n",
    "                           y= sample_test.max_y[missing_cluster_p], name = 'Missing Cluster Peaks', showlegend=True),row = 1, col=1)\n",
    "\n",
    "comp_bandpass.add_trace(go.Scatter(mode='markers',marker=dict(size=7,color='#00FCF5'),x = cluster_sample.pks_time[missing_pt_p] , y= cluster_sample.pks_values[missing_pt_p], \n",
    "                          name = 'Missing Current Version Peaks', showlegend=True),row = 1, col=1)\n",
    "\n",
    "comp_bandpass.add_trace(go.Scatter(x = sample_test.bp_x,y = sample_test.bp_y,showlegend=False),row=2, col=1)\n",
    "comp_bandpass.add_trace(go.Scatter(x = recreated_cluster.bp_x,y = recreated_cluster.bp_y,showlegend=False),row=3, col=1)\n",
    "\n",
    "comp_bandpass.update_layout(xaxis = dict(tickmode = 'array', tickvals = cluster_sample.ticks_values, ticktext = cluster_sample.time_format_axis),\n",
    "                            xaxis2 = dict(tickmode = 'array', tickvals = sample_test.ticks_values, ticktext = sample_test.time_format_axis),\n",
    "                            xaxis3 = dict(tickmode = 'array', tickvals = recreated_cluster.ticks_values, ticktext = recreated_cluster.time_format_axis))\n",
    "comp_bandpass.update_layout(xaxis_range=[141300,142900],xaxis2_range=[141300,142900],yaxis_range=[-2.8,3],yaxis2_range=[-0.2,0.2])\n",
    "comp_bandpass.update_layout(height=1200,width = 1200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a29207-ace4-4c09-ad06-56695b58c310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5861fb10-11ca-4bde-8a9a-b0d7d760d2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf29da-7894-4770-82ba-24da442ce41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
